# üìñ QUICK REFERENCE - File Storage System

## üéØ M·ªôt c√¢u l·ªánh, m·ªôt m·ª•c ƒë√≠ch

---

## üöÄ CRAWL DATA

```bash
# Test (10 docs, 5 ph√∫t)
python3 run_government_crawl.py --max-docs 10

# Small (50 docs, 15 ph√∫t)
python3 run_government_crawl.py --max-docs 50

# Medium (100 docs, 30 ph√∫t)
python3 run_government_crawl.py --max-docs 100

# Large (500 docs, 2-3 gi·ªù)
python3 run_government_crawl.py --max-docs 500
```

---

## üëÄ XEM DATA

```bash
# Menu t∆∞∆°ng t√°c
python3 view_data.py

# Xem t√≥m t·∫Øt
python3 view_data.py --summary

# Xem 10 docs m·ªõi nh·∫•t
python3 view_data.py --recent 10

# Xem chi ti·∫øt 1 doc
python3 view_data.py --id "doc_id_here"

# List PDFs
python3 view_data.py --pdfs

# List texts
python3 view_data.py --texts

# Check data t·ªìn t·∫°i
python3 view_data.py --check
```

---

## üì§ EXPORT DATA

```bash
# Export JSON
python3 tools/export_data.py --format json

# Export JSON (tech only)
python3 tools/export_data.py --format json --tech-only

# Export CSV
python3 tools/export_data.py --format csv

# Export both
python3 tools/export_data.py --format both

# Generate report
python3 tools/export_data.py --report
```

---

## üìÅ TRUY C·∫¨P FILES

```bash
# Xem metadata
cat data/metadata.json | python3 -m json.tool | less

# Xem metadata (pretty)
cat data/metadata.json | python3 -m json.tool

# Count documents
cat data/metadata.json | python3 -c "import json,sys; print(len(json.load(sys.stdin)))"

# List PDFs
ls -lh data/pdf_documents/

# List texts
ls -lh data/text_documents/

# Open PDF folder
open data/pdf_documents/          # Mac
nautilus data/pdf_documents/      # Linux

# View a text file
cat data/text_documents/filename.txt

# Preview text
head -50 data/text_documents/filename.txt
```

---

## üîç SEARCH

```bash
# Search trong metadata
grep "c√¥ng ngh·ªá" data/metadata.json

# Search trong text files
grep -r "AI" data/text_documents/

# Search v√† show filename
grep -l "blockchain" data/text_documents/*.txt

# Search case-insensitive
grep -ri "chuy·ªÉn ƒë·ªïi s·ªë" data/text_documents/

# Count matches
grep -r "c√¥ng ngh·ªá" data/text_documents/ | wc -l
```

---

## üìä STATISTICS

```bash
# Quick stats
python3 view_data.py --summary

# Detailed report
python3 tools/export_data.py --report

# Count PDFs
ls data/pdf_documents/*.pdf | wc -l

# Count texts
ls data/text_documents/*.txt | wc -l

# Total PDF size
du -sh data/pdf_documents/

# Total text size
du -sh data/text_documents/
```

---

## üîß MAINTENANCE

```bash
# Backup everything
cp -r data/ backup_$(date +%Y%m%d)/

# Backup metadata only
cp data/metadata.json backup/metadata_$(date +%Y%m%d).json

# Compress for sharing
zip -r data_$(date +%Y%m%d).zip data/

# Clean and recrawl
rm data/metadata.json
python3 run_government_crawl.py --max-docs 10

# View logs
tail -f logs/vietnamese_legal_crawler.log

# View errors only
tail -f logs/vietnamese_legal_crawler_errors.log
```

---

## üß™ TESTING

```bash
# Test file storage
python3 test_file_storage.py

# Test with small crawl
python3 run_government_crawl.py --max-docs 5

# Verify data
python3 view_data.py --check
```

---

## üíª PYTHON INTEGRATION

### **Load metadata:**

```python
import json

with open('data/metadata.json', 'r') as f:
    metadata = json.load(f)

print(f"Total: {len(metadata)}")
```

---

### **Filter tech docs:**

```python
import json

with open('data/metadata.json', 'r') as f:
    metadata = json.load(f)

tech_docs = {
    doc_id: doc 
    for doc_id, doc in metadata.items() 
    if doc.get('is_tech_related')
}

print(f"Tech docs: {len(tech_docs)}")
```

---

### **Read text file:**

```python
from pathlib import Path

doc_id = "Nghi_dinh_15_2024_abc123"
text_file = Path(f"data/text_documents/{doc_id}.txt")

if text_file.exists():
    with open(text_file, 'r') as f:
        content = f.read()
    print(content[:500])
```

---

### **Use FileStorageManager:**

```python
from src.storage.file_storage import FileStorageManager

storage = FileStorageManager()

# Get stats
stats = storage.get_statistics()
print(f"Total: {stats['total_documents']}")

# Get all docs
docs = storage.get_all_documents()

# Get one doc
doc = storage.get_document("doc_id_here")

# Export
storage.export_to_json(Path("export.json"))
```

---

## üìÇ FILE LOCATIONS

```
data/
‚îú‚îÄ‚îÄ metadata.json               # All document info
‚îú‚îÄ‚îÄ pdf_documents/              # Original PDFs
‚îÇ   ‚îî‚îÄ‚îÄ *.pdf
‚îî‚îÄ‚îÄ text_documents/             # Converted texts
    ‚îî‚îÄ‚îÄ *.txt

exports/
‚îú‚îÄ‚îÄ tech_documents.json         # Exported JSON
‚îú‚îÄ‚îÄ tech_documents.csv          # Exported CSV
‚îî‚îÄ‚îÄ report_*.txt                # Reports

logs/
‚îú‚îÄ‚îÄ vietnamese_legal_crawler.log         # All logs
‚îî‚îÄ‚îÄ vietnamese_legal_crawler_errors.log  # Errors only
```

---

## üéØ COMMON WORKFLOWS

### **Workflow 1: Quick test**

```bash
python3 run_government_crawl.py --max-docs 10
python3 view_data.py --summary
python3 tools/export_data.py --format json
cat exports/tech_documents.json | python3 -m json.tool
```

---

### **Workflow 2: Production crawl**

```bash
python3 run_government_crawl.py --max-docs 100
python3 view_data.py --summary
python3 tools/export_data.py --report
python3 tools/export_data.py --format both
```

---

### **Workflow 3: Inspect data**

```bash
python3 view_data.py
# Choose option 2 (summary)
# Choose option 3 (recent docs)
# Choose option 4 (detail by ID)
```

---

### **Workflow 4: Share with colleague**

```bash
# Create archive
zip -r legal_docs_$(date +%Y%m%d).zip data/

# Share the zip file
# Colleague just unzips and has everything!
```

---

## ‚ö° ONE-LINERS

```bash
# Total documents
cat data/metadata.json | python3 -c "import json,sys; print(len(json.load(sys.stdin)))"

# Tech documents
cat data/metadata.json | python3 -c "import json,sys; d=json.load(sys.stdin); print(sum(1 for x in d.values() if x['is_tech_related']))"

# Documents by year 2024
cat data/metadata.json | python3 -c "import json,sys; d=json.load(sys.stdin); print(sum(1 for x in d.values() if x.get('issued_date','').startswith('2024')))"

# List all titles
cat data/metadata.json | python3 -c "import json,sys; d=json.load(sys.stdin); [print(x['title']) for x in d.values()]"

# Documents from vanban.chinhphu.vn
cat data/metadata.json | python3 -c "import json,sys; d=json.load(sys.stdin); print(sum(1 for x in d.values() if 'vanban.chinhphu.vn' in x.get('source_website','')))"
```

---

## üÜò TROUBLESHOOTING

### **Problem: No data**

```bash
# Solution: Crawl first
python3 run_government_crawl.py --max-docs 10
```

---

### **Problem: Metadata file corrupted**

```bash
# Solution: Delete and recrawl
rm data/metadata.json
python3 run_government_crawl.py --max-docs 10
```

---

### **Problem: Can't find document ID**

```bash
# Solution: List all IDs
cat data/metadata.json | python3 -c "import json,sys; d=json.load(sys.stdin); [print(k) for k in d.keys()]"
```

---

### **Problem: Want to recrawl**

```bash
# Solution: Just crawl again, duplicates are skipped
python3 run_government_crawl.py --max-docs 50
```

---

## üìû HELP

```bash
# Crawler help
python3 run_government_crawl.py --help

# Viewer help
python3 view_data.py --help

# Export help
python3 tools/export_data.py --help
```

---

## üéä SUMMARY

| Task | Command |
|------|---------|
| **Crawl** | `python3 run_government_crawl.py --max-docs N` |
| **View** | `python3 view_data.py` |
| **Summary** | `python3 view_data.py --summary` |
| **Export** | `python3 tools/export_data.py --format json` |
| **Report** | `python3 tools/export_data.py --report` |
| **Metadata** | `cat data/metadata.json \| python3 -m json.tool` |
| **Backup** | `cp -r data/ backup/` |
| **Share** | `zip -r data.zip data/` |

---

**Bookmarks:**
- `FILE_STORAGE_GUIDE.md` - Complete guide
- `QUICK_START.md` - Getting started
- `THAY_DOI_QUAN_TRONG.md` - What changed

**Simple. Fast. Effective.** üìÅ‚ú®
