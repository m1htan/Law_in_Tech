# ğŸš€ PRODUCTION MODE - Crawl ToÃ n Bá»™ VÄƒn Báº£n PhÃ¡p Luáº­t VN

## ğŸ“Š Tá»•ng quan

**Mode**: Production - Systematic Crawling  
**Target**: ToÃ n bá»™ vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam  
**Focus**: CÃ´ng nghá»‡ thÃ´ng tin (CNTT)  
**Method**: Category-based crawling (khÃ´ng dá»±a vÃ o keyword)

---

## ğŸ¯ Má»¥c tiÃªu

Crawl **toÃ n bá»™** vÄƒn báº£n phÃ¡p luáº­t VN bao gá»“m:
1. âœ… VÄƒn báº£n quy pháº¡m phÃ¡p luáº­t
2. âœ… VÄƒn báº£n dÆ°á»›i luáº­t  
3. âœ… Nghá»‹ quyáº¿t, Nghá»‹ Ä‘á»‹nh, Quyáº¿t Ä‘á»‹nh, ThÃ´ng tÆ°
4. âœ… Luáº­t, PhÃ¡p lá»‡nh
5. âœ… Dá»± tháº£o vÄƒn báº£n

**Æ¯u tiÃªn**: Máº£ng CÃ´ng nghá»‡ thÃ´ng tin

---

## ğŸ—‚ï¸ Database Schema

### **Table: documents**

```sql
- id (PRIMARY KEY)
- title, document_number, document_type
- issuing_agency, signer
- issued_date, effective_date
- summary, full_text
- source_url, source_website
- pdf_path, text_path
- category, subject, keywords
- is_tech_related, tech_categories, relevance_score
- status, superseded_by
- crawled_at, updated_at
- ai_analyzed, ai_analysis
```

### **Table: crawl_progress**

```sql
- website, category
- last_page, total_pages
- documents_crawled
- started_at, updated_at, completed
```

**Location**: `data/legal_documents.db` (SQLite)

---

## ğŸ—ï¸ Architecture

### **Components**:

1. **DatabaseManager** (`src/database/models.py`)
   - SQLite database
   - Document model
   - CRUD operations
   - Statistics & progress tracking

2. **ProductionCrawler** (`src/crawlers/production_crawler.py`)
   - Systematic category crawling
   - Pagination handling
   - Progress saving
   - Resume capability

3. **Export Tools** (`tools/export_data.py`)
   - JSON export
   - CSV export
   - Summary reports

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### **Mode 1: Limited Crawl (Testing)**

```bash
# Crawl 50 documents, 10 pages
python3 run_production_crawl.py --mode limited \
  --category cong-nghe-thong-tin \
  --max-docs 50 \
  --max-pages 10
```

**Output:**
- ~50 documents crawled
- Saved to database
- PDFs downloaded & converted
- Progress tracked

**Time**: ~10-15 minutes

### **Mode 2: Full Crawl (Production)**

```bash
# Crawl ALL tech documents
python3 run_production_crawl.py --mode full
```

**Output:**
- ALL tech categories crawled:
  - CÃ´ng nghá»‡ thÃ´ng tin
  - ThÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­
  - Sá»Ÿ há»¯u trÃ­ tuá»‡
  - Viá»…n thÃ´ng
- Thousands of documents
- Complete database

**Time**: Several hours (depends on bandwidth)

âš ï¸ **Warning**: This will crawl A LOT of data!

### **Mode 3: Resume Crawl**

```bash
# Resume interrupted crawl
python3 run_production_crawl.py --mode resume \
  --category cong-nghe-thong-tin
```

**Features:**
- Automatically resumes from last page
- No duplicate crawling
- Progress preserved

---

## ğŸ“ Categories Available

### **Tech Categories** (Priority):

1. **CÃ´ng nghá»‡ thÃ´ng tin** (`cong-nghe-thong-tin`)
   - Priority: HIGH
   - URL: `/van-ban/Cong-nghe-thong-tin`
   - Expected: 1000+ documents

2. **ThÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­** (`thuong-mai-dien-tu`)
   - Priority: HIGH
   - URL: `/van-ban/Thuong-mai`
   - Expected: 500+ documents

3. **Sá»Ÿ há»¯u trÃ­ tuá»‡** (`so-huu-tri-tue`)
   - Priority: MEDIUM
   - URL: `/van-ban/So-huu-tri-tue`
   - Expected: 300+ documents

4. **Viá»…n thÃ´ng** (`vien-thong`)
   - Priority: MEDIUM
   - Included in CNTT category

---

## ğŸ“Š Export Data

### **Export Tech Documents (JSON)**:

```bash
python3 tools/export_data.py --tech-only --format json
```

**Output**: `exports/tech_documents.json`

### **Export All Data (CSV)**:

```bash
python3 tools/export_data.py --format csv
```

**Output**: `exports/legal_docs_TIMESTAMP.csv`

### **Generate Report**:

```bash
python3 tools/export_data.py --report
```

**Output**:
```
VIETNAMESE LEGAL DOCUMENTS CRAWLER
Summary Report
============================================================

Total Documents: 150
Tech Documents: 120
Tech Ratio: 80.0%

DOCUMENTS BY TYPE
------------------------------------------------------------
Nghá»‹ Ä‘á»‹nh: 45
ThÃ´ng tÆ°: 38
Quyáº¿t Ä‘á»‹nh: 25
...
```

---

## ğŸ”„ Workflow

### **Production Crawl Process:**

```
1. [Category Selection]
        â†“
2. [Page Crawling]
   - Get document list from page
   - Extract: title, number, type, date
        â†“
3. [Document Detail]
   - Crawl each document page
   - Extract full details
   - Download PDFs
   - Convert to text
        â†“
4. [Database Save]
   - Insert/update document
   - Save progress
        â†“
5. [Next Page/Document]
   - Continue until all pages
   - Or max limit reached
        â†“
6. [Export & Report]
   - Export to JSON/CSV
   - Generate statistics
```

---

## ğŸ“¦ Data Structure

### **LegalDocument Model**:

```python
{
    "id": 123,
    "title": "Nghá»‹ Ä‘á»‹nh 123/2024/NÄ-CP vá»...",
    "document_number": "123/2024/NÄ-CP",
    "document_type": "Nghá»‹ Ä‘á»‹nh",
    "issuing_agency": "ChÃ­nh phá»§",
    "issued_date": "2024-10-14",
    "effective_date": "2024-12-01",
    "summary": "Quy Ä‘á»‹nh vá»...",
    "full_text": "...",
    "source_url": "https://...",
    "pdf_path": "data/pdf_documents/...",
    "text_path": "data/text_documents/...",
    "category": "CÃ´ng nghá»‡ thÃ´ng tin",
    "is_tech_related": true,
    "relevance_score": 9.5,
    "status": "active"
}
```

---

## ğŸ¯ Features

### âœ… **Systematic Crawling**
- No keyword dependency
- Complete category coverage
- Pagination handling
- All document types

### âœ… **Progress Tracking**
- Save progress after each page
- Resume from interruption
- Track documents crawled
- Prevent duplicates

### âœ… **Data Persistence**
- SQLite database
- Structured schema
- Fast queries
- Easy export

### âœ… **PDF Processing**
- Auto download PDFs
- Convert to text (3 methods)
- Store both formats
- Metadata extraction

### âœ… **Classification**
- Auto detect tech-related
- Category assignment
- Relevance scoring
- Status tracking

---

## ğŸ“ˆ Expected Results

### **Limited Crawl (50 docs)**:

```
Pages crawled: ~10
Documents found: ~50-100
Documents saved: ~50
PDFs downloaded: ~25-50
Time: ~15 minutes
Database size: ~50 MB
```

### **Full Crawl (All tech)**:

```
Categories: 4
Pages crawled: ~200+
Documents found: ~2000+
Documents saved: ~2000+
PDFs downloaded: ~1000+
Time: ~6-10 hours
Database size: ~2-5 GB
```

---

## ğŸ› ï¸ Maintenance

### **Check Progress**:

```python
from src.database.models import DatabaseManager

db = DatabaseManager()
stats = db.get_statistics()
print(stats)
```

### **Query Database**:

```python
# Get tech documents
tech_docs = db.get_tech_documents(limit=10)

# Get specific document
doc = db.get_document_by_url("https://...")
```

### **Database Location**:

```
data/legal_documents.db
```

**Tools**: SQLite Browser, DBeaver, etc.

---

## âš™ï¸ Configuration

File: `.env`

```env
# Crawl settings
MAX_CONCURRENT_REQUESTS=3
REQUEST_DELAY=2
CRAWL_TIMEOUT=60000
MAX_RETRIES=3

# Date range
START_YEAR=2022
END_YEAR=2024
```

---

## ğŸ” Example Usage

### **Scenario 1: Quick Test**

```bash
# Test with 10 documents
python3 run_production_crawl.py --mode limited \
  --max-docs 10 --max-pages 2
```

### **Scenario 2: One Category Full Crawl**

```bash
# Crawl all CNTT documents
python3 run_production_crawl.py --mode limited \
  --category cong-nghe-thong-tin \
  --max-pages 100 --max-docs 1000
```

### **Scenario 3: Production (All)**

```bash
# Full production crawl
python3 run_production_crawl.py --mode full
```

### **Scenario 4: Export Results**

```bash
# Export to JSON and CSV
python3 tools/export_data.py --tech-only --format both

# Generate report
python3 tools/export_data.py --report
```

---

## ğŸ“Š Statistics & Monitoring

### **Real-time Progress**:

Logs show:
```
[1/50] Document: Nghá»‹ Ä‘á»‹nh 123/2024...
âœ“ Document saved: ID 456
Progress: Page 5/10, Documents: 25/50
```

### **Database Stats**:

```python
stats = db.get_statistics()
# {
#   'total_documents': 150,
#   'tech_documents': 120,
#   'by_type': {'Nghá»‹ Ä‘á»‹nh': 45, ...},
#   'by_year': {'2024': 80, ...}
# }
```

---

## ğŸ¯ Next Steps

### **After Initial Crawl**:

1. âœ… **Analyze Data**
   - Check database stats
   - Review documents
   - Verify completeness

2. âœ… **AI Analysis** (Optional)
   - Run AI agent on saved docs
   - Add relevance scores
   - Generate summaries

3. âœ… **Export & Use**
   - Export to needed formats
   - Use in research
   - Present findings

4. âœ… **Schedule Updates**
   - Regular re-crawls
   - New documents detection
   - Status updates

---

## ğŸš¨ Important Notes

### **Rate Limiting**:
- Delay: 2 seconds between requests
- Respect robots.txt
- Avoid overloading servers

### **Storage**:
- Database can grow large (GB+)
- PDFs take significant space
- Monitor disk usage

### **Network**:
- Stable internet required
- Several hours for full crawl
- Can resume if interrupted

### **Legal**:
- Public documents only
- Respect copyright
- Academic/research use

---

## ğŸŠ Summary

You now have a **PRODUCTION-READY** system that can:

âœ… Crawl toÃ n bá»™ vÄƒn báº£n phÃ¡p luáº­t VN  
âœ… Focus on CNTT systematically  
âœ… Store in structured database  
âœ… Track progress & resume  
âœ… Download & process PDFs  
âœ… Export to multiple formats  
âœ… Generate statistics & reports  

**Status**: Ready for large-scale crawling!

---

## ğŸš€ Quick Start

```bash
# 1. Test with limited crawl
python3 run_production_crawl.py --mode limited --max-docs 10

# 2. Check database
python3 tools/export_data.py --report

# 3. Export data
python3 tools/export_data.py --tech-only --format json

# 4. Ready for production!
python3 run_production_crawl.py --mode full
```

---

**Date**: 2025-10-14  
**Status**: âœ… PRODUCTION READY  
**Mode**: Systematic Crawling  
**Target**: Complete Legal Document Collection

ğŸ‰ **Ready to crawl!**
