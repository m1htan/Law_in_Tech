#!/usr/bin/env python3
"""
Script ƒë·ªÉ ch·∫°y crawler v·ªõi c·∫•u h√¨nh m·ªõi t·∫≠p trung v√†o c√°c trang .gov.vn
v·ªÅ c√¥ng ngh·ªá th√¥ng tin, chuy·ªÉn ƒë·ªïi s·ªë v√† khoa h·ªçc c√¥ng ngh·ªá
"""

import os
import sys
import json
import orjson
from datetime import datetime

# Th√™m src v√†o path ƒë·ªÉ import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from crawlers.crawl4ai_runner import run_crawl

def load_seeds():
    """Load danh s√°ch URL seeds t·ª´ config"""
    config_path = os.path.join("config", "seeds.json")
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)

def save_results(results, output_dir="outputs/jsonl"):
    """L∆∞u k·∫øt qu·∫£ crawl v√†o file JSONL"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Group results by domain
    by_domain = {}
    for result in results:
        domain = result.get("source_site", "unknown")
        if domain not in by_domain:
            by_domain[domain] = []
        by_domain[domain].append(result)
    
    # Save each domain to separate file
    for domain, domain_results in by_domain.items():
        filename = f"{domain}.jsonl"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, "wb") as f:
            for result in domain_results:
                f.write(orjson.dumps(result, option=orjson.OPT_INDENT_2) + b"\n")
        
        print(f"ƒê√£ l∆∞u {len(domain_results)} k·∫øt qu·∫£ t·ª´ {domain} v√†o {filepath}")

def main():
    """Main function"""
    print("üöÄ B·∫Øt ƒë·∫ßu crawl c√°c trang .gov.vn v·ªÅ c√¥ng ngh·ªá th√¥ng tin v√† chuy·ªÉn ƒë·ªïi s·ªë...")
    print(f"‚è∞ Th·ªùi gian b·∫Øt ƒë·∫ßu: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Load seeds
        seeds = load_seeds()
        print(f"üìã ƒê√£ load {len(seeds)} URL seeds:")
        for i, seed in enumerate(seeds, 1):
            print(f"   {i}. {seed}")
        
        # Run crawler
        print("\nüîç B·∫Øt ƒë·∫ßu crawl...")
        results = run_crawl(seeds)
        
        print(f"\n‚úÖ Ho√†n th√†nh crawl! T√¨m th·∫•y {len(results)} k·∫øt qu·∫£.")
        
        if results:
            # Save results
            save_results(results)
            
            # Print summary
            print("\nüìä T√≥m t·∫Øt k·∫øt qu·∫£:")
            domains = {}
            doc_types = {}
            for result in results:
                domain = result.get("source_site", "unknown")
                doc_type = result.get("doc_type", "unknown")
                domains[domain] = domains.get(domain, 0) + 1
                doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
            
            print("   Theo domain:")
            for domain, count in sorted(domains.items()):
                print(f"     - {domain}: {count} t√†i li·ªáu")
            
            print("   Theo lo·∫°i t√†i li·ªáu:")
            for doc_type, count in sorted(doc_types.items()):
                print(f"     - {doc_type}: {count} t√†i li·ªáu")
        else:
            print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o ph√π h·ª£p v·ªõi ti√™u ch√≠.")
            
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y crawler: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    print(f"\n‚è∞ Th·ªùi gian k·∫øt th√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    return 0

if __name__ == "__main__":
    exit(main())